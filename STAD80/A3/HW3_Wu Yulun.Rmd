---
title: "STAD80: Homework #3"
author: "Yulun Wu"
date: "Due: 2023-03-12"
header-includes:
   - \usepackage{amsmath}
output:
  pdf_document:
    keep_tex: true
    toc: yes
    toc_depth: 2
---

## Question 4 (30 Points) Big Data. Big Profit.

### Answer: 

Q1
```{r,warning=FALSE,message=FALSE}
library('fastDummies')
load("q1.RData")

# Create dummy variables
dataTrainAll = dummy_cols(dataTrainAll, select_columns = c("Region","City","AdX","Domain","Key_Page","Ad_Vis","Ad_Form"),
remove_first_dummy = TRUE, ignore_na = TRUE, remove_selected_columns = FALSE
)

# Column bind dummy variables
dataTrainAll$Region = cbind(dataTrainAll$Region_3,dataTrainAll$Region_6)
dataTrainAll$City = cbind(dataTrainAll$City_2,dataTrainAll$City_3,dataTrainAll$City_4,dataTrainAll$City_5,dataTrainAll$City_6)
dataTrainAll$AdX = cbind(dataTrainAll$AdX_2,dataTrainAll$AdX_3)
dataTrainAll$Domain = cbind(dataTrainAll$Domain_5KFUl5p0Gxsvgmd4wspENpn,dataTrainAll$Domain_trqRTu5Jg9q9wMKYvmpENpn,dataTrainAll$Domain_trqRTudNXqN8ggc4JKTI,dataTrainAll$`Domain_trqRTuT-GNTYJNKbuKz`)
dataTrainAll$Key_Page = cbind(dataTrainAll$Key_Page_9f4e2f16b6873a7eb504df6f61b24044,dataTrainAll$Key_Page_df6f61b2409f4e2f16b6873a7eb50444)
dataTrainAll$Ad_Vis = cbind(dataTrainAll$Ad_Vis_1,dataTrainAll$Ad_Vis_2)
dataTrainAll$Ad_Form = dataTrainAll$Ad_Form_1

# Standardized 3 columns
dataTrainAll$Ad_Width = (dataTrainAll$Ad_Width - mean(dataTrainAll$Ad_Width))/sd(dataTrainAll$Ad_Width)
dataTrainAll$Ad_Height = (dataTrainAll$Ad_Height - mean(dataTrainAll$Ad_Height))/sd(dataTrainAll$Ad_Height)
dataTrainAll$Floor_Price = (dataTrainAll$Floor_Price - mean(dataTrainAll$Floor_Price))/sd(dataTrainAll$Floor_Price)

# Put needed features in a dataframe
dataTrain = cbind(dataTrainAll$Region,dataTrainAll$City,dataTrainAll$AdX,dataTrainAll$Domain,dataTrainAll$Ad_Width,dataTrainAll$Ad_Height,dataTrainAll$Ad_Vis,dataTrainAll$Ad_Form,dataTrainAll$Floor_Price,dataTrainAll$Key_Page)

# Modify Click column
dataTrainAll$Click[dataTrainAll$Click >= 1] = 1
```
From the definition of dummy_cols, remove_first_dummy = TRUE will remove the first dummy category in each column. In Region, Region 1 will be baseline category. In City, City 1 will be baseline category. In AdX, AdX 1 will be baseline category.In Domain, Domain 5Fa-expoBTTR1m58uG will be baseline category. In Key_Page, Key_Page 3a7eb50444df6f61b2409f4e2f16b687 will be baseline category. In Ad_Vis, Ad_Vis 0 will be baseline category. In Ad_Form, Ad_Form 0 will be baseline category.

Part a
```{r,warning=FALSE}
library(glmnet)
# Fit LASSO and Ridge model
fit.lasso = glmnet(dataTrain,dataTrainAll$Click,family="binomial",standardize=FALSE,alpha=1)
fit.ridge = glmnet(dataTrain,dataTrainAll$Click,family="binomial",standardize=FALSE,alpha=0)

# Plot graph
plot(fit.lasso,main="Lasso’s Regularization Path",xlab="L1 Norm",ylab="Coefficients")
plot(fit.ridge,main="Ridge’s Regularization Path",xlab="L1 Norm",ylab="Coefficients")
```

Part b
```{r,warning=FALSE}
# Coefficients of LASSO model in last column of beta
dim_lassobeta = dim(fit.lasso$beta)
abs(fit.lasso$beta[,dim_lassobeta[2]])

# Coefficients of Ridge model in last column of beta
dim_ridgebeta = dim(fit.ridge$beta)
abs(fit.ridge$beta[,dim_ridgebeta[2]])
```
According to Google, the importance of features in LASSO and Ridge Regression are indicated by the absolute value of corresponding coefficient. The larger the absolute value of coefficient, the more important the feature is. 

In LASSO model, there are 3 features have coefficients such that the absolute value of coefficient is greater than 1, ordering them in the decreasing order of importance (absolute value of coefficients), they are: Ad_Width, Ad_Height ,Domain="trqRTuT-GNTYJNKbuKz".

In Ridge model, there are 2 features have coefficients such that the absolute value of coefficient is greater than 1, ordering them in the decreasing order of importance (absolute value of coefficients), they are: Ad_Width, Ad_Height.

Part c
```{r,warning=FALSE}
# Fit LASSO and Ridge 5 folds model
cv.lasso = cv.glmnet(dataTrain,dataTrainAll$Click,family="binomial",standardize=FALSE,alpha=1,nfolds = 5)

cv.ridge = cv.glmnet(dataTrain,dataTrainAll$Click,family="binomial",standardize=FALSE,alpha=0,nfolds = 5)

par(mfrow=c(1,2))
plot(fit.lasso,main="Lasso’s Regularization Path",xlab="L1 Norm",ylab="Coefficients")
abline(v=sum(abs(coef(cv.lasso,s="lambda.min")[2:22])))
plot(cv.lasso,main="Cross Validation for LASSO",xlab="log(Lambda)",ylab="Binomial Deviance")

par(mfrow=c(1,2))
plot(fit.ridge,main="Ridge’s Regularization Path",xlab="L1 Norm",ylab="Coefficients")
abline(v=sum(abs(coef(cv.ridge,s="lambda.min")[2:22])))
plot(cv.ridge,main="Cross Validation for Ridge",xlab="log(Lambda)",ylab="Binomial Deviance")
```
According to Part b and regularization path graph, 3 features play a more important role in LASSO model when predicting whether there is at least 1 click. The cross validation plot of LASSO Regression shows that lambda that gives minimum cvm is around log(lambda)=-8, and the model has around 16 non-zero features with lambda that gives minimum cvm. 

According to Part b and regularization path graph, 2 features play a more important role in Ridge model when predicting whether there is at least 1 click. The cross validation plot of Ridge Regression shows that lambda that gives minimum cvm is around log(lambda)=-5, and the model has around 21 non-zero features with lambda that gives minimum cvm.

The reason why models with larger degrees of freedom do not necessarily do better in the cross validation is because larger degrees of freedom can cause overfitting, and models that overfit have higher cvm (mean cross-validated error) in cross validation, and cross validation wants to find a lambda such that cvm is minimized.

Part d
```{r,warning=FALSE}
# Modify dataTest in the way model wants
# Create dummy variables
dataTest = dummy_cols(dataTest, select_columns = c("Region","City","AdX","Domain","Key_Page","Ad_Vis","Ad_Form"),
remove_first_dummy = TRUE, ignore_na = TRUE, remove_selected_columns = FALSE
)

# Column bind dummy variables
dataTest$Region = cbind(dataTest$Region_3,dataTest$Region_6)
dataTest$City = cbind(dataTest$City_2,dataTest$City_3,dataTest$City_4,dataTrainAll$City_5,dataTest$City_6)
dataTest$AdX = cbind(dataTest$AdX_2,dataTest$AdX_3)
dataTest$Domain = cbind(dataTest$Domain_5KFUl5p0Gxsvgmd4wspENpn,dataTest$Domain_trqRTu5Jg9q9wMKYvmpENpn,dataTest$Domain_trqRTudNXqN8ggc4JKTI,dataTest$`Domain_trqRTuT-GNTYJNKbuKz`)
dataTest$Key_Page = cbind(dataTest$Key_Page_9f4e2f16b6873a7eb504df6f61b24044,dataTest$Key_Page_df6f61b2409f4e2f16b6873a7eb50444)
dataTest$Ad_Vis = cbind(dataTest$Ad_Vis_1,dataTest$Ad_Vis_2)
dataTest$Ad_Form = dataTest$Ad_Form_1

# Standardized 3 columns
dataTest$Ad_Width = (dataTest$Ad_Width - mean(dataTest$Ad_Width))/sd(dataTest$Ad_Width)
dataTest$Ad_Height = (dataTest$Ad_Height - mean(dataTest$Ad_Height))/sd(dataTest$Ad_Height)
dataTest$Floor_Price = (dataTest$Floor_Price - mean(dataTest$Floor_Price))/sd(dataTest$Floor_Price)

# Put needed features in a dataframe
dataTest_modified = cbind(dataTest$Region,dataTest$City,dataTest$AdX,dataTest$Domain,dataTest$Ad_Width,dataTest$Ad_Height,dataTest$Ad_Vis,dataTest$Ad_Form,dataTest$Floor_Price,dataTest$Key_Page)

# Modify Click column
dataTestRes$Click[dataTestRes$Click >= 1] = 1
```

```{r,warning=FALSE}
# Number of rows in dataTestRes
n = dim(dataTestRes)
n = n[1]
# Test for LASSO model
pred_lasso = predict(cv.lasso,dataTest_modified,s="lambda.min")
predAndTruth.lasso = cbind(pred_lasso,dataTestRes$Click) # bind prediction and truth
# True yi = 1, but prediction is 0
nrow(predAndTruth.lasso[dataTestRes$Click==1&pred_lasso<0,])/n
# True yi = 0, but prediction is 1
nrow(predAndTruth.lasso[dataTestRes$Click==0&pred_lasso>0,])/n

# Test for Ridge model
pred_ridge = predict(cv.ridge,dataTest_modified,s="lambda.min")
predAndTruth.ridge = cbind(pred_ridge,dataTestRes$Click) # bind prediction and truth
# True yi = 1, but prediction is 0
nrow(predAndTruth.ridge[dataTestRes$Click==1&pred_ridge<0,])/n
# True yi = 0, but prediction is 1
nrow(predAndTruth.ridge[dataTestRes$Click==0&pred_ridge>0,])/n
```

Q2
```{r,warning=FALSE}
load("q1.RData")

# Standardize 3 columns
dataTrainAll$AdX = (dataTrainAll$AdX - mean(dataTrainAll$AdX))/sd(dataTrainAll$AdX)
dataTrainAll$iPinYou_Bid = (dataTrainAll$iPinYou_Bid - mean(dataTrainAll$iPinYou_Bid))/sd(dataTrainAll$iPinYou_Bid)
dataTrainAll$Comp_Bid = (dataTrainAll$Comp_Bid - mean(dataTrainAll$Comp_Bid))/sd(dataTrainAll$Comp_Bid)
dataTrain2 = cbind(dataTrainAll$AdX,dataTrainAll$iPinYou_Bid)
```

```{r,warning=FALSE}
# Fit linear model
fit.linear = lm(Comp_Bid~AdX+iPinYou_Bid,data=dataTrainAll)
fit.linear$coefficients[2:3] # MLE coefficients of linear model
# Fit LASSO model
fit.lasso = glmnet(dataTrain2,dataTrainAll$Comp_Bid,family="gaussian",standardize = FALSE,alpha=1)
half_L1norm = 0.5*sum(abs(fit.linear$coefficients[2:3]))
half_L1norm # 0.5*L1 norm of MLE coefficients of linear model
beta_leq_half_L1norm = fit.lasso$beta[,colSums(abs(fit.lasso$beta))<=half_L1norm] # All beta that have L1 norm <= half_L1norm
lasso.coef = beta_leq_half_L1norm[,dim(beta_leq_half_L1norm)[2]] # Beta that have max L1 norm among beta_leq_half_L1norm
lasso.coef

beta1 = seq(-.5,1,length.out=100)
beta2 = seq(-.5,1,length.out=100)
beta = expand.grid(beta1, beta2) # Expand a grid of all combinations of beta1 and beta2

# Function to calculate MSE of a single combination of beta1 and beta2
calc_MSE = function(beta1,beta2){
  return(sum((dataTrainAll$Comp_Bid-beta1*dataTrainAll$AdX-beta2*dataTrainAll$iPinYou_Bid)^2))
}

MSE = mapply(calc_MSE,beta[,1],beta[,2]) # Use mapply to apply calc_MSE to all combinations
MSE = matrix(MSE,100,100)
L1_lasso = sum(abs(lasso.coef))
contour(beta1,beta2,MSE,xlab="beta1",ylab="beta2") # Contour
points(fit.linear$coefficients[2],fit.linear$coefficients[3],col="red") # MLE coefficients
points(lasso.coef[1],lasso.coef[2],col="blue3") # LASSO coefficients
plot(function(x){x+L1_lasso},-L1_lasso,0,add=T,col="green")
plot(function(x){-x-L1_lasso},-L1_lasso,0,add=T,col="green")
plot(function(x){x-L1_lasso},0,L1_lasso,add=T,col="green")
plot(function(x){-x+L1_lasso},0,L1_lasso,add=T,col="green")

# Ridge Regression part
library(wordspace)
library(DescTools)
fit.ridge = glmnet(dataTrain2,dataTrainAll$Comp_Bid,family="gaussian",standardize=FALSE,alpha=0)
half_L2norm = 0.5*norm(fit.linear$coefficients[2:3], type = "2")   
half_L2norm # 0.5*L2 norm of MLE coefficients of linear model
beta_leq_half_L2norm = fit.ridge$beta[,colNorms(fit.ridge$beta)<=half_L2norm] # All beta that have L2 norm <= half_L2norm
ridge.coef = beta_leq_half_L2norm[,dim(beta_leq_half_L2norm)[2]] # Beta that have max L2 norm among beta_leq_half_L2norm
ridge.coef
L2_ridge = norm(ridge.coef, type = "2") 
contour(beta1,beta2,MSE,xlab="beta1",ylab="beta2") # Contour
points(fit.linear$coefficients[2],fit.linear$coefficients[3],col="red") # MLE coefficients
points(ridge.coef[1],ridge.coef[2],col="blue3") # Ridge coefficients
DrawEllipse(0,0, radius.x = L2_ridge, radius.y = L2_ridge,border="green",col =NA)
```
The MLE coefficients sit at the center of the level curve. MLE coefficients minimize MSE. LASSO and Ridge coefficients sit close to the most inner contour curve (5000), It is the point on L1 or L2 ball that has minimum MSE. LASSO and Ridge try to minimize MSE while coefficients satisfy L1/L2 norm. LASSO encourages sparse because L1 ball is diamond shape, so the L1 diamond have greater chance to have minimum MSE at 4 corner point, and at corner point, there is one coefficient equal to 0 (in 2d case). A simple example is, when you rotate a circle for 360 degree, during the rotation, any point on circle can be a maximum point at some angle with equal occurrence; but if you rotate a diamond, 4 corner point have further higher frequency to become maximum points than others. 

## Question 5 (20 Points) Discover Gravitational Waves in Your Room 

### Answer: 

```{r,warning=FALSE}
set.seed(10)
LIGO = read.table("LIGO.Hanford.Data.txt", header=F)
```
Q1
```{r,warning=FALSE,eval=FALSE}
n_row = nrow(LIGO)
sqrt_n_row = sqrt(n_row)
C = matrix(, nrow = n_row, ncol = n_row)
for (j in 1:n_row){
  for (k in 1:n_row){
    if (j==1){
      C[j,k] = sqrt_n_row
    }
    else {
      C[j,k] = sqrt(2/n_row)*cos(pi*(2*k-1)*(j-1)/(2*n_row))
    }
  }
}

cv.lasso = cv.glmnet(C,as.matrix(LIGO[1]),alpha=1,nfolds = 10)
#w_hat = cv.lasso$beta[,cv.lasso$lambda.min]
#cv.lasso$beta
y_pred = predict(cv.lasso,C,s="lambda.min")
#plot(cv.lasso$beta,)
cv.lasso$beta
```

Q2


## Question 6 (25 Points) Upright Human Detection in Photos

### Answer: 

(a)
```{r,warning=FALSE}
library(png)
source("functions.R")
subdir_pos = "/pngdata/pos/"
subdir_neg = "/pngdata/neg/"
filename_pos = "5"
filename_neg = "7"
photo_pos = readPNG(paste(file.path(getwd(),subdir_pos,filename_pos), ".png", sep = ""))
photo_neg = readPNG(paste(file.path(getwd(),subdir_neg,filename_neg), ".png", sep = ""))
# Transfer to the black and white version
photo_pos = rgb2gray(photo_pos)
writePNG(photo_pos, target = "bwpos.png")
photo_neg = rgb2gray(photo_neg)
writePNG(photo_neg, target = "bwneg.png")

photo_neg = crop.r(photo_neg,160,96) # Randomly crop
writePNG(photo_neg, target = "cropneg.png")

photo_pos.grad = grad(photo_pos, 128, 64, F)
setEPS()
postscript("gradpos.eps")
g=grad(photo_pos, 128, 64, T)
dev.off()
photo_neg.grad = grad(photo_neg, 128, 64, F)
setEPS()
postscript("gradneg.eps")
g=grad(photo_neg, 128, 64, T)
dev.off()

photo_pos.hog = hog(photo_pos.grad$xgrad, photo_pos.grad$ygrad, 4, 4, 6)
photo_neg.hog = hog(photo_neg.grad$xgrad, photo_neg.grad$ygrad, 4, 4, 6)
```
```{r echo=FALSE, out.width ='80%', warning = FALSE}

knitr::include_graphics(paste(file.path(getwd(),subdir_pos,filename_pos), ".png", sep = ""))
knitr::include_graphics("bwpos.png")
knitr::include_graphics("gradpos.eps")
photo_pos.hog

knitr::include_graphics(paste(file.path(getwd(),subdir_neg,filename_neg), ".png", sep = ""))
knitr::include_graphics("bwneg.png")
knitr::include_graphics("cropneg.png")
knitr::include_graphics("gradneg.eps")
photo_neg.hog
```

(b)
```{r,warning=FALSE,message=FALSE}
getwd()
n=500
features_pos = data.frame(matrix(ncol = 96, nrow = 0))
features_neg = data.frame(matrix(ncol = 96, nrow = 0))
for (i in 1:500){
  photo_pos = readPNG(paste(file.path(getwd(),subdir_pos,as.character(i)), ".png", sep = ""))
  photo_neg = readPNG(paste(file.path(getwd(),subdir_neg,as.character(i)), ".png", sep = ""))
  # Transfer to the black and white version
  photo_pos = rgb2gray(photo_pos)
  photo_neg = rgb2gray(photo_neg)
  
  photo_neg = crop.r(photo_neg,160,96) # Randomly crop

  photo_pos.grad = grad(photo_pos, 128, 64, F)
  photo_neg.grad = grad(photo_neg, 128, 64, F)

  photo_pos.hog = hog(photo_pos.grad$xgrad, photo_pos.grad$ygrad, 4, 4, 6)
  photo_neg.hog = hog(photo_neg.grad$xgrad, photo_neg.grad$ygrad, 4, 4, 6)
  
  features_pos[i,] = photo_pos.hog
  features_neg[i,] = photo_neg.hog
}
features_data = rbind(features_pos,features_neg) # Row bind two dataframe together, row 1 to 500 are POS, 501 to 1000 are NEG
features_data[1:500,"POS"] = 1 # 1 indicates POS
features_data[501:1000,"POS"] = 0 # 0 indicates NEG
```

Part II
```{r,warning=FALSE}
fit.logit = glmnet(features_data[,1:96],features_data$POS,family="binomial")
plot(fit.logit,main="Logistic's Regularization Path",xlab="",ylab="Coefficients")
fit.logit.cv = cv.glmnet(as.matrix(features_data[,1:96]),features_data$POS,family="binomial",type.measure="class")
plot(fit.logit.cv,main="Cross Validation for Logistic",xlab="Lambda",ylab="Cross Validation Errors (CVM)")
```

## Question 7 (30 Points) Sentiment Analysis on Amazon Product Reviews 

### Answer: 

(a)
```{r,warning=FALSE}
library(plyr)
load("Amazon_SML.RData")
colnames(dat) # Column names
nrow(dat) # Number of reviews
length(unique(dat$name)) # Number of unique products

# Product has the most ‘5’ ratings 
prod_r5 = dat[dat$rating==5,] # Product has ‘5’ ratings 
count_r5 = count(prod_r5$name) # Count of 5 ratings with respect to names
count_r5

# Product has the most ‘1’ ratings 
prod_r1 = dat[dat$rating==1,] # Product has ‘1’ ratings 
count_r1 = count(prod_r1$name) # Count of 1 ratings with respect to names
count_r1[count_r1$freq==max(count_r1$freq),]
```
Column names are name,review and rating. There are 1312 reviews. There are 20 unique products. Vulli Sophie the Giraffe Teether has the most ‘5’ ratings which is 526. Infant Optics DXR-5 2.4 GHz Digital Video Baby Monitor with Night Vision has the most ‘5’ ratings which is 68.

(b)
```{r}
unique(dat$rating) # List of unique rating values
nrow_r1 = sum(count_r1$freq) # Number of reviews for rating=1
nrow_r1
nrow_r5 = sum(count_r5$freq) # Number of reviews for rating=5
nrow_r5

source("tdMat.R")
```
There are 656 reviews of each rating value in the entire dataset. The possible rating values are 1 and 5. The best performance of a ”constant classifier” is 0.5, because we have half of data with rating=1, and half with rating=5, so a ”constant classifier” will get 50% of calssification correct.

(c)
```{r,warning=FALSE,message=FALSE}
source("splitData.R")
set.seed(10)
lambda<-exp(seq(-20, -1, length.out = 99))
cvfit<-cv.glmnet(train.x,train.y,family="binomial",type.measure="class",lambda=lambda)
fit.logit = glmnet(train.x,train.y,family="binomial",lambda = cvfit$lambda.1se)
fit.logit$df # The number of nonzero coefficients 

# Twenty words with most positive coefficients
beta_de = fit.logit$beta[order(fit.logit$beta,decreasing = T),] # Sort beta in decreasing order
names(beta_de[1:20]) # 20 words with most positive coefficients

# Twenty words with most negative coefficients
beta_in = fit.logit$beta[order(fit.logit$beta),] # Sort beta in increasing order
names(beta_in[1:20]) # 20 words with most negative coefficients
```
The number of covariates that have non-zero coefficients in the model selected by lambda.1se are 353. 

(d)
```{r,warning=FALSE}
# This function will compute number of row in train.x such that column rname has value>0
count_doc=function(rname){
  return(nrow(train.x[train.x[,rname]>0,]))
}

count_pos = mapply(count_doc,names(beta_de[1:20]))
count_pos # Covariates name and frequency, these covariates are 20 words with most positive coefficients, covariates are listed in decreasing beta order

count_neg = mapply(count_doc,names(beta_in[1:20]))
count_neg # Covariates name and frequency, these covariates are 20 words with most negative coefficients, covariates are listed in increasing beta order

# Number of documents using "love" had a rating of 1 or 5
length(train.y[train.y==1&train.x[,"love"]>0]) # Number of documents using "love" had a rating of 5
length(train.y[train.y==0&train.x[,"love"]>0]) # Number of documents using "love" had a rating of 1
using_love = train.tag[train.x[,"love"]>0] # Tag of documents used in training set that has "love"
dat[using_love[1],"review"] # The first document that used this most positive word "love"

# Number of documents using "wast" had a rating of 1 or 5
length(train.y[train.y==1&train.x[,"wast"]>0]) # Number of documents using "wast" had a rating of 5
length(train.y[train.y==0&train.x[,"wast"]>0]) # Number of documents using "wast" had a rating of 1
using_wast = train.tag[train.x[,"wast"]>0] # Tag of documents used in training set that has "wast"
dat[using_wast[1],"review"] # The first document that used this most negative word "wast"
```
The word with the most positive weights that appeared in more than 10 documents is "love". The word with the most negative weights that appeared in more than 10 documents is "wast". Among 427 of documents in train.x that using "love", 364 of them have rating of 5, 63 of them have rating of 1. Among 86 of documents in train.x that using "wast", 6 of them have rating of 5, 80 of them have rating of 1. 

(e)
```{r,warning=FALSE}
n_test = nrow(test.x) # Number of rows in testing set
pred_y=predict(fit.logit,test.x) # Predict testing set
# Number of predictions where pred_y don't agree with test.y divided by number of rows in testing set, this is misclassification rate
length(pred_y[(test.y==1&pred_y<0)|((test.y==0&pred_y>0)),])/n_test
```
The misclassification rate is 0.07575758. This is definitely better than the “constant classifier”, because the misclassification rate for “constant classifier” is 0.5.

## Question 8 (15 Points) Identifying Risk Factors for the Heart Disease

### Answer: 

Part I
```{r,warning=FALSE}
heart = read.csv("framingham.csv",header = T)
n = nrow(heart)
x = heart[,-ncol(heart)] # dataframe with out TenYearCHD
fit.logit = glm(heart$TenYearCHD~as.matrix(x),family = binomial)
summary_fit = summary(fit.logit)
# The statistically significant coefficients
summary_fit$coefficients[summary_fit$coefficients[ ,4] < 0.05,1] 
```

Part II
```{r,warning=FALSE}
set.seed(100)

# First subset
sub_dataset_ind = sample(rownames(heart),n/5,replace=F)
sub_dataset1 = heart[sub_dataset_ind,]
rest = heart[-as.numeric(sub_dataset_ind),]

# Second subset
sub_dataset_ind = sample(rownames(rest),n/5,replace=F)
sub_dataset2 = rest[sub_dataset_ind,]
rest = rest[-as.numeric(sub_dataset_ind),]

# Third subset
sub_dataset_ind = sample(rownames(rest),n/5,replace=F)
sub_dataset3 = rest[sub_dataset_ind,]
rest = rest[-as.numeric(sub_dataset_ind),]

# Fourth subset
sub_dataset_ind = sample(rownames(rest),n/5,replace=F)
sub_dataset4 = rest[sub_dataset_ind,]
rest = rest[-as.numeric(sub_dataset_ind),]

# Fifth subset
sub_dataset_ind = sample(rownames(rest),n/5,replace=F)
sub_dataset5 = rest[sub_dataset_ind,]

train1 = rbind(sub_dataset1,sub_dataset2,sub_dataset3,sub_dataset4)
train1.x = train1[,-ncol(heart)]
fit.logit1 = glm(train1$TenYearCHD~as.matrix(train1.x),family = binomial)
test.x = sub_dataset5[,-ncol(heart)]
pred_y_test = exp(as.matrix(test.x)%*%fit.logit1$coefficients[2:16]+fit.logit1$coefficients[1])/(1+exp(as.matrix(test.x)%*%fit.logit1$coefficients[2:16]+fit.logit1$coefficients[1])) # Idk why but predict works weird here, so have to compute it manually
length(pred_y_test[(sub_dataset5$TenYearCHD==1&pred_y_test<0.5)|((sub_dataset5$TenYearCHD==0&pred_y_test>0.5))])/(nrow(heart)/5)
```
Here I find some NA in dataframe, I didn't remove NAs because handout didn't ask so. The prediction error of logistic regression (misclassification rate) is 0.2488208.

Part III
```{r,warning=FALSE}
heart = na.omit(heart)
x = heart[,-ncol(heart)] # dataframe with out TenYearCHD
fit.lasso = glmnet(x,heart$TenYearCHD,family="binomial",alpha=1)
x = model.matrix(TenYearCHD~., heart)[,-1]
fit.lasso.cv = cv.glmnet(x,heart$TenYearCHD,family="binomial",type.measure="class",nfolds = 5)

plot(fit.lasso.cv,main="Cross Validation for LASSO",xlab="Lambda",ylab="Cross Validation Errors (CVM)")
```
The curve is not a typical U-curve. I don't think we need regularization here, because CVM doesn't change much with different $\lambda$.













 




















